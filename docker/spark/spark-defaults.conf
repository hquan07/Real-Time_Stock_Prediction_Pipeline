###############################################
# Spark Application Configuration
# For PySpark 4.0.1 â€” Kafka + Postgres + ML

# ----------------------------
# Spark Master / Deploy Mode
spark.master                     spark://spark-master:7077
spark.submit.deployMode          client

# ----------------------------
# Application Name
spark.app.name                   yfinance_stream_processor

# ----------------------------
# Python version & environment
spark.pyspark.python             python3
spark.pyspark.driver.python      python3

# ----------------------------
# Logging level
spark.eventLog.enabled           true
spark.eventLog.dir               file:///tmp/spark-events
spark.sql.shuffle.partitions     200

# ----------------------------
# Kafka Integration
spark.kafka.bootstrap.servers    kafka:9092
spark.streaming.stopGracefullyOnShutdown true

# Enable automatic backpressure for streaming
spark.streaming.backpressure.enabled   true
spark.streaming.kafka.maxRatePerPartition 1000

# ----------------------------
# Serialization / Performance
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired  false

# ----------------------------
# Postgres JDBC Connection
spark.jars.packages              org.postgresql:postgresql:42.7.3,\
                                 org.apache.spark:spark-sql-kafka-0-10_2.12:4.0.1,\
                                 org.apache.kafka:kafka-clients:3.7.0

# ----------------------------
# Memory & Executor Config
spark.driver.memory              2g
spark.executor.memory            2g
spark.executor.instances         2
spark.executor.cores             2

# ----------------------------
# Optimization
spark.sql.adaptive.enabled       true
spark.sql.files.maxPartitionBytes 134217728
spark.memory.fraction           0.7

# ----------------------------
# Warehouse / Temp Storage
spark.sql.warehouse.dir          /tmp/spark-warehouse
spark.local.dir                  /tmp/spark